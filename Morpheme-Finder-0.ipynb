{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morpheme Finder\n",
    "[TOC]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Define Env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from requests import request, ConnectionError\n",
    "from json import loads\n",
    "from random import sample\n",
    "from math import ceil\n",
    "import pycrfsuite\n",
    "import re\n",
    "\n",
    "word_dict_morpholex = defaultdict(None)\n",
    "word_dict_celex = defaultdict(None)\n",
    "label_func = defaultdict(None)\n",
    "known_prefixes = set()\n",
    "known_suffixes = set()\n",
    "\n",
    "EVQR_AFFIX = '<evqr.affix>'\n",
    "PREFIX_AND_SUFFIX = '<prefix.and.suffix>'\n",
    "VOWEL = '<vowel>'\n",
    "CELEX_WORD_ROOT = '<celex.word.root>'\n",
    "morpholex = '<morphoLEX>'\n",
    "\n",
    "CROSS_VALIDATION_FOLD = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('.env.json') as f:\n",
    "        ENV_VARIABLES = loads(f.read())\n",
    "        f.close()\n",
    "except FileNotFoundError:\n",
    "    ENV_VARIABLES = {'DATA_DIR': 'C:\\\\'}\n",
    "DATA_DIR = ENV_VARIABLES['DATA_DIR']\n",
    "FTP_DIR = 'http://m106.nthu.edu.tw/~s106062341/morpheme_finder_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Word:\n",
    "\n",
    "    @staticmethod\n",
    "    def create_synonym_postfix(word, delete=None, append=None):\n",
    "        return f'{word}{f\"--{delete}--\" if delete is not None else \"\"}{f\"++{append}++\" if append is not None else \"\"}'\n",
    "\n",
    "    @staticmethod\n",
    "    def create_synonym_prefix(word, delete=None, append=None):\n",
    "        return f'{f\"--{delete}--\" if delete is not None else \"\"}{f\"++{append}++\" if append is not None else \"\"}{word}'\n",
    "\n",
    "    @staticmethod\n",
    "    def letter_cmp(a, b):\n",
    "        divider = 0\n",
    "        for i, (letter_a, letter_b) in enumerate(zip(a, b)):\n",
    "            if letter_a != letter_b:\n",
    "                divider = i\n",
    "        return min(divider, len(a), len(b))\n",
    "\n",
    "    def __init__(self, text, affix_list):\n",
    "        self.text = text\n",
    "        self.affix_list = affix_list\n",
    "        self.synonym = defaultdict(None)\n",
    "        self.label = defaultdict(None)\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        return sum([c for c in self.synonym.values()])\n",
    "\n",
    "    def create_label(self, label_name, *args):\n",
    "        if label_name not in label_func:\n",
    "            return False\n",
    "        self.label[label_name] = label_func[label_name](self, *args)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Accessing\n",
    "### first provide a method to access files either in local storage or in FTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_file(filename: str, callback: classmethod) -> bool:\n",
    "    try:\n",
    "        with open(f'asd{DATA_DIR}{filename}', 'r') as f:\n",
    "            callback(f.read())\n",
    "            f.close()\n",
    "            return True\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            res = request('GET', f'{FTP_DIR}{filename}')\n",
    "            res.encoding = 'Big5'\n",
    "            callback(res.text)\n",
    "            return True\n",
    "        except ConnectionError:\n",
    "            print('HTTP connection failed')\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f'Load failed: {e}')\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "includes:\n",
    "1. *EVQR.word.and.affix.txt'*\n",
    "2. *prefixes.txt*\n",
    "3. *suffixes.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def evqr_word_and_suffix_callback(content):\n",
    "#     for line in content.split('\\n')[1:-1]:\n",
    "#         word, *affix_list = line.replace('-', '').split(' ')[:-1]\n",
    "#         word_dict[word] = (Word(word, affix_list))\n",
    "# if get_file('EVQR.word.and.affix.txt', evqr_word_and_suffix_callback):\n",
    "#     print('Load done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def celex_word_and_root_callback(content):\n",
    "#     for line in content.split('\\r\\n'):\n",
    "#         word, *affix_list = line.split(' ')\n",
    "#         word_dict[word] = (Word(word, affix_list))\n",
    "# if get_file('CELEX.word.and.root.txt', celex_word_and_root_callback):\n",
    "#     print('Load done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load CELEX.word.and.root.txt done [11770 / 8296]\n"
     ]
    }
   ],
   "source": [
    "bad_celex = []\n",
    "def celex_word_and_root_callback(content):\n",
    "    for line in content.split('\\r\\n'):\n",
    "        word, *affix_list = line.split(' ')\n",
    "        if word == ''.join(affix_list):\n",
    "            word_dict_celex[word] = Word(word, affix_list)\n",
    "        else:\n",
    "            bad_celex.append(line)\n",
    "if get_file('CELEX.word.and.root.txt', celex_word_and_root_callback):\n",
    "    print(f'Load CELEX.word.and.root.txt done [{len(word_dict_celex.keys())} / {len(bad_celex)}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Load morphoLEX.txt done [27025 / 41589]\n"
     ]
    }
   ],
   "source": [
    "bad_morpholex = []\n",
    "def morpholex_callback(content):\n",
    "    for line in content.split('\\r\\n'):\n",
    "        word, *affix_list = line.split('\\t')\n",
    "        try:\n",
    "            affix_list = affix_list[0].split()\n",
    "        except:\n",
    "            print(affix_list)\n",
    "        if word == ''.join(affix_list):\n",
    "            word_dict_morpholex[word] = Word(word, affix_list)\n",
    "        else:\n",
    "            bad_morpholex.append(line)\n",
    "if get_file('morphoLEX.txt', morpholex_callback):\n",
    "    print(f'Load morphoLEX.txt done [{len(word_dict_morpholex.keys())} / {len(bad_morpholex)}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load prefixes & suffixes done\n"
     ]
    }
   ],
   "source": [
    "def prefix_callback(content):\n",
    "    for line in content.split('\\n')[1:-1]:\n",
    "        known_prefixes.update(filter(lambda x: len(x) > 0, line[:-1].strip().replace('-', '').split(', ')))\n",
    "\n",
    "def suffix_callback(content):\n",
    "    for line in content.split('\\n'):\n",
    "        known_suffixes.update(filter(lambda x: len(x) > 0, line[:-1].strip().replace('-', '').split(', ')))\n",
    "\n",
    "if get_file('prefixes_1.txt', prefix_callback) and get_file('suffixes.txt', suffix_callback) and get_file('prefixes.txt', prefix_callback) and get_file('all_suffixes.txt', suffix_callback):\n",
    "    print('Load prefixes & suffixes done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word_roots.txt done\n"
     ]
    }
   ],
   "source": [
    "def word_roots_callback(content):\n",
    "    for line in content.split('\\n'):\n",
    "        prefixes = line.split('\\t')[0].split(', ')\n",
    "        prefixes = [re.sub(r'[^A-z]', '', prefix) for prefix in prefixes]\n",
    "        if '' not in prefixes:\n",
    "            known_prefixes.update(prefixes)\n",
    "if get_file('word_roots.txt', word_roots_callback):\n",
    "    print('Load word_roots.txt done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prefix_callback(content):\n",
    "#     for line in content.split('\\n')[1:-1]:\n",
    "#         known_prefixes.update(filter(lambda x: len(x) > 0, line[:-1].strip().replace('-', '').split(', ')))\n",
    "        \n",
    "# if get_file('prefixes_1.txt', prefix_callback) and get_file('prefixes.txt', prefix_callback):\n",
    "#     print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelize Word\n",
    "### Mapping Label Function\n",
    "because different label has its label function respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping done\n"
     ]
    }
   ],
   "source": [
    "def evqr_affix(word):\n",
    "    text = word.text\n",
    "    label = [0] * len(text)\n",
    "    pos = 0\n",
    "    for affix in word.affix_list:\n",
    "        if affix.lower() in text:\n",
    "            label[text.find(affix, pos)] = 1 if pos != 0 else 0\n",
    "            pos = text.find(affix, pos) + len(affix)\n",
    "        else:\n",
    "            k = Word.letter_cmp(text[pos:], affix)\n",
    "            if k > 1:\n",
    "                label[pos] = 1 if pos != 0 else 0\n",
    "                pos += 1\n",
    "\n",
    "    return [t for t in zip(text, label)]\n",
    "\n",
    "def vowel(word):\n",
    "    vowels = {\"a\", \"e\", \"i\", \"o\", \"u\"}\n",
    "    return [(letter, int(letter in vowels)) for letter in word.text]\n",
    "\n",
    "def prefix_and_suffix(word):\n",
    "    word_len = len(word.text)\n",
    "    label = [0] * word_len\n",
    "\n",
    "    for i in range(word_len):\n",
    "        pattern = word.text[:word_len - 1 - i]\n",
    "        if pattern in known_prefixes:\n",
    "            label[len(pattern)] = 1\n",
    "\n",
    "    for i in range(word_len):\n",
    "        pattern = word.text[i + 1:]\n",
    "        if pattern in known_suffixes:\n",
    "            label[i] = 2 if label[i] == 0 else 3\n",
    "\n",
    "    return [t for t in zip(word.text, label)]\n",
    "\n",
    "def celex_word_root(word):\n",
    "    text = word.text\n",
    "    label = [0] * len(text)\n",
    "    pos = 0\n",
    "    for affix in word.affix_list:\n",
    "        prev_pos = text.find(affix, pos)\n",
    "        label[prev_pos] = 1 if pos != 0 else 0\n",
    "        pos = prev_pos + len(affix)\n",
    "\n",
    "    return [t for t in zip(text, label)]\n",
    "\n",
    "label_func[EVQR_AFFIX] = evqr_affix\n",
    "label_func[VOWEL] = vowel\n",
    "label_func[PREFIX_AND_SUFFIX] = prefix_and_suffix\n",
    "label_func[CELEX_WORD_ROOT] = celex_word_root\n",
    "label_func[morpholex] = celex_word_root\n",
    "print('Mapping done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Label for each Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 27025/27025 [00:00<00:00, 445408.29it/s]\n",
      "100%|█████████████████████████████████| 11770/11770 [00:00<00:00, 411024.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for word in tqdm(word_dict_morpholex.values()):\n",
    "    if not word.create_label(morpholex):\n",
    "        print('Failed at combining labels')\n",
    "        \n",
    "for word in tqdm(word_dict_celex.values()):\n",
    "    if not word.create_label(morpholex):\n",
    "        print('Failed at combining labels')\n",
    "print('Label done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(f'labeled by EVQR.word.and.suffix: ignoble -> {word_dict[\"ignoble\"].label[EVQR_AFFIX]}')\n",
    "# print(f'labeled by prefix & suffix     : demagog -> {word_dict[\"demagog\"].label[PREFIX_AND_SUFFIX]}')\n",
    "# print(f'labeled by position of vowels  : amphibology -> {word_dict[\"amphibology\"].label[VOWEL]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dict = {**word_dict_morpholex, **word_dict_celex}\n",
    "word_dict = word_dict_celex\n",
    "# word_dict = word_dict_morpholex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 11770/11770 [00:00<00:00, 1582426.45it/s]\n"
     ]
    }
   ],
   "source": [
    "prepared_word = []\n",
    "for word in tqdm(word_dict.values()):\n",
    "    prepared_word.append(word.label[morpholex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training\n",
    "### features creator\n",
    "based on\n",
    "1. prev & after letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_char_features(word, i):\n",
    "    \n",
    "    text = ''\n",
    "    length = len(word)\n",
    "    for j in range(length):\n",
    "        text += word[j][0]\n",
    "    pre = text[:i]\n",
    "    suf = text[i:]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'char=' + word[i][0],\n",
    "        'vowel=1' if word[i][0] in ['a', 'e', 'i', 'o', 'u'] else 'vowel=0', \n",
    "        \n",
    "        'prefix=1' if pre in known_prefixes else 'prefix=0', \n",
    "        'suffix=1' if suf in known_suffixes else 'suffix=0' \n",
    "    ]\n",
    "    \n",
    "#     not from the beginning nor til the end\n",
    "    if i > 0:\n",
    "        for j in range(i+1, length):\n",
    "            if text[i:j] in known_prefixes or text[i:j] in known_suffixes:\n",
    "                features.extend([\n",
    "                    'potential_morpheme'\n",
    "                ])\n",
    "                break\n",
    "    \n",
    "\n",
    "    if i >= 1:\n",
    "        features.extend([\n",
    "            'char-1=' + word[i-1][0],\n",
    "            'char-1:0=' + word[i-1][0] + word[i][0],\n",
    "        ])\n",
    "    else:\n",
    "        features.append(\"BOS\")\n",
    "\n",
    "    if i >= 2:\n",
    "        features.extend([\n",
    "            'char-2=' + word[i-2][0],\n",
    "            'char-2:0=' + word[i-2][0] + word[i-1][0] + word[i][0],\n",
    "            'char-2:-1=' + word[i-2][0] + word[i-1][0],\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "    if i + 1 < len(word):\n",
    "        features.extend([\n",
    "            'char+1=' + word[i+1][0],\n",
    "            'char:+1=' + word[i][0] + word[i+1][0],\n",
    "        ])\n",
    "    else:\n",
    "        features.append(\"EOS\")\n",
    "        \n",
    "        \n",
    "    if i + 2 < len(word):\n",
    "        features.extend([\n",
    "            'char+2=' + word[i+2][0],\n",
    "            'char:+2=' + word[i][0] + word[i+1][0] + word[i+2][0],\n",
    "            'char+1:+2=' + word[i+1][0] + word[i+2][0],\n",
    "        ])\n",
    "\n",
    "\n",
    "    if i + 3 < len(word):\n",
    "        features.extend([\n",
    "            'char+3=' + word[i+3][0], \n",
    "            'char:+3=' + word[i][0] + word[i+1][0] + word[i+2][0] + word[i+3][0],\n",
    "            'char+1:+3=' + word[i+1][0] + word[i+2][0] + word[i+3][0], \n",
    "            'char+2:+3=' + word[i+2][0] + word[i+3][0]\n",
    "        ])\n",
    "        \n",
    "    if i >= 3:\n",
    "        features.extend([\n",
    "            'char-3=' + word[i-3][0], \n",
    "            'char-3:0=' + word[i-3][0] + word[i-2][0] + word[i-1][0] + word[i][0],\n",
    "            'char-3:-1=' + word[i-3][0] + word[i-2][0] + word[i-1][0],\n",
    "            'char-3:-2=' + word[i-3][0] + word[i-2][0]\n",
    "        ])\n",
    "        \n",
    "    if i + 4 < len(word):\n",
    "        features.extend([\n",
    "            'char+4=' + word[i+4][0], \n",
    "            'char:+4=' + word[i][0] + word[i+1][0] + word[i+2][0] + word[i+3][0] + word[i+4][0],\n",
    "            'char+1:+4=' + word[i+1][0] + word[i+2][0] + word[i+3][0] + word[i+4][0], \n",
    "            'char+2:+4=' + word[i+2][0] + word[i+3][0] + word[i+4][0], \n",
    "            'char+3:+4=' + word[i+3][0] + word[i+4][0]\n",
    "        ])\n",
    "        \n",
    "    if i >= 4:\n",
    "        features.extend([\n",
    "            'char-4=' + word[i-4][0], \n",
    "            'char-4:0=' + word[i-4][0] + word[i-3][0] + word[i-2][0] + word[i-1][0] + word[i][0],\n",
    "            'char-4:-1=' + word[i-4][0] + word[i-3][0] + word[i-2][0] + word[i-1][0],\n",
    "            'char-4:-2=' + word[i-4][0] + word[i-3][0] + word[i-2][0], \n",
    "            'char-4:-3=' + word[i-4][0] + word[i-3][0]\n",
    "        ])\n",
    "        \n",
    "    if i + 5 < len(word):\n",
    "        features.extend([\n",
    "            'char+5=' + word[i+5][0], \n",
    "            'char:+5=' + word[i][0] + word[i+1][0] + word[i+2][0] + word[i+3][0] + word[i+4][0] + word[i+5][0],\n",
    "            'char+1:+5=' + word[i+1][0] + word[i+2][0] + word[i+3][0] + word[i+4][0] + word[i+5][0], \n",
    "            'char+2:+5=' + word[i+2][0] + word[i+3][0] + word[i+4][0] + word[i+5][0], \n",
    "            'char+3:+5=' + word[i+3][0] + word[i+4][0] + word[i+5][0], \n",
    "            'char+4:+5=' + word[i+4][0] + word[i+5][0],\n",
    "        ])\n",
    "        \n",
    "    if i >= 5:\n",
    "        features.extend([\n",
    "            'char-5=' + word[i-5][0], \n",
    "            'char-5:0=' + word[i-5][0] + word[i-4][0] + word[i-3][0] + word[i-2][0] + word[i-1][0] + word[i][0],\n",
    "            'char-5:-1=' + word[i-5][0] + word[i-4][0] + word[i-3][0] + word[i-2][0] + word[i-1][0],\n",
    "            'char-5:-2=' + word[i-5][0] + word[i-4][0] + word[i-3][0] + word[i-2][0], \n",
    "            'char-5:-3=' + word[i-5][0] + word[i-4][0] + word[i-3][0], \n",
    "            'char-5:-4=' + word[i-5][0] + word[i-4][0],\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    if i + 6 < len(word):\n",
    "        features.extend([\n",
    "            'char+6=' + word[i+6][0], \n",
    "            'char:+6=' + word[i][0] + word[i+1][0] + word[i+2][0] + word[i+3][0] + word[i+4][0] + word[i+5][0] + word[i+6][0],\n",
    "            'char+1:+6=' + word[i+1][0] + word[i+2][0] + word[i+3][0] + word[i+4][0] + word[i+5][0] + word[i+6][0], \n",
    "            'char+2:+6=' + word[i+2][0] + word[i+3][0] + word[i+4][0] + word[i+5][0] + word[i+6][0], \n",
    "            'char+3:+6=' + word[i+3][0] + word[i+4][0] + word[i+5][0] + word[i+6][0], \n",
    "            'char+4:+6=' + word[i+4][0] + word[i+5][0] + word[i+6][0],\n",
    "            'char+5:+6=' + word[i+5][0] + word[i+6][0],\n",
    "        ])\n",
    "        \n",
    "    if i >= 6:\n",
    "        features.extend([\n",
    "            'char-6=' + word[i-6][0], \n",
    "            'char-6:0=' + word[i-6][0] + word[i-5][0] + word[i-4][0] + word[i-3][0] + word[i-2][0] + word[i-1][0] + word[i][0],\n",
    "            'char-6:-1=' + word[i-6][0] + word[i-5][0] + word[i-4][0] + word[i-3][0] + word[i-2][0] + word[i-1][0],\n",
    "            'char-6:-2=' + word[i-6][0] + word[i-5][0] + word[i-4][0] + word[i-3][0] + word[i-2][0], \n",
    "            'char-6:-3=' + word[i-6][0] + word[i-5][0] + word[i-4][0] + word[i-3][0], \n",
    "            'char-6:-4=' + word[i-6][0] + word[i-5][0] + word[i-4][0],\n",
    "            'char-6:-5=' + word[i-6][0] + word[i-5][0],\n",
    "        ])\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def create_word_features(prepared_word):\n",
    "    return [create_char_features(prepared_word, i) for i in range(len(prepared_word))]\n",
    "\n",
    "\n",
    "def create_word_labels(prepared_word):\n",
    "    return [str(part[1]) for part in prepared_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### create k-fold cross validation\n",
    "we split all data into 5 folds here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_range = set(range(len(word_dict.values())))\n",
    "sample_set_size = ceil(len(sample_range) / CROSS_VALIDATION_FOLD)\n",
    "sample_list = []\n",
    "selected_samples = set()\n",
    "for i in range(CROSS_VALIDATION_FOLD - 1):\n",
    "    samples = set(sample(sample_range, sample_set_size))\n",
    "    sample_list.append(samples)\n",
    "    sample_range.difference_update(samples)\n",
    "sample_list.append(set(sample_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for l in sample_list:\n",
    "#     print(' ')\n",
    "#     print(list(sorted(l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### interface of using pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(folds):\n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "    for fold in tqdm(folds):\n",
    "        for idx in fold:\n",
    "            trainer.append(create_word_features(prepared_word[idx]),\n",
    "                           create_word_labels(prepared_word[idx]))\n",
    "\n",
    "    trainer.set_params({\n",
    "        'c1': 0.1,\n",
    "        'c2': 1e-3,\n",
    "        'max_iterations': 100,\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    trainer.train('word-segmentation.crfsuite')\n",
    "\n",
    "\n",
    "def test(fold):\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open('word-segmentation.crfsuite')\n",
    "    score = 0\n",
    "    for word in fold:\n",
    "        w = word.replace(\" \", \"\")\n",
    "        prediction = tagger.tag(create_word_features(w))\n",
    "        complete = \"\"\n",
    "        for i, p in enumerate(prediction):\n",
    "            if int(p) >= 1:\n",
    "                complete += \" \" + w[i]\n",
    "            else:\n",
    "                complete += w[i]\n",
    "        if complete == ' '.join(word_dict[word].affix_list):\n",
    "            score += 1\n",
    "        else:\n",
    "            continue\n",
    "#             print(f'{word} -> {complete}, {word_dict[word].affix_list}')\n",
    "    return score / len(fold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implement\n",
    "run 5 times of train & test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.15it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.19it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8725573491928632, 0.8598130841121495, 0.8666100254885302, 0.8776550552251486, 0.8619371282922684]\n",
      "0.867714528462192\n"
     ]
    }
   ],
   "source": [
    "word_list = list(word_dict.values())\n",
    "scores = []\n",
    "for test_set_idx in range(len(sample_list)):\n",
    "    test_fold = [word_list[idx].text for idx in sample_list[test_set_idx]]\n",
    "    train_folds = sample_list[:test_set_idx] + sample_list[(test_set_idx+1):]\n",
    "    train(train_folds)\n",
    "    scores.append(test(test_fold))\n",
    "print(scores)\n",
    "print(sum(scores) / 5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('word-segmentation.crfsuite')\n",
    "\n",
    "def segment_word(word):\n",
    "    w = word.replace(\" \", \"\")\n",
    "    prediction = tagger.tag(create_word_features(w))\n",
    "    complete = \"\"\n",
    "    for i, p in enumerate(prediction):\n",
    "        if int(p) >= 1:\n",
    "            complete += \" \" + w[i]\n",
    "        else:\n",
    "            complete += w[i]\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'segment ation'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_word('segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seg ment'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_word('segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inter sect'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_word('intersect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pre view'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_word('preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict_for_test = word_dict_celex\n",
    "word_dict_for_train = word_dict_morpholex\n",
    "model_name = 'morpholex'\n",
    "\n",
    "# word_dict_for_test = word_dict_morpholex\n",
    "# word_dict_for_train = word_dict_celex\n",
    "# model_name = 'celex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 11770/11770 [00:00<00:00, 2327312.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# pure word\n",
    "word_for_test = []\n",
    "for word in tqdm(word_dict_for_test.values()):\n",
    "    word_for_test.append(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aback'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_for_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 27025/27025 [00:00<00:00, 1879941.38it/s]\n"
     ]
    }
   ],
   "source": [
    "word_for_train = []\n",
    "for word in tqdm(word_dict_for_train.values()):\n",
    "    word_for_train.append(word.label[morpholex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0), ('l', 0), ('f', 0)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_for_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for idx in range(len(word_for_train)):\n",
    "    trainer.append(create_word_features(word_for_train[idx]),\n",
    "                   create_word_labels(word_for_train[idx]))\n",
    "\n",
    "trainer.set_params({\n",
    "    'c1': 0.1,\n",
    "    'c2': 1e-3,\n",
    "    'max_iterations': 50,\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "trainer.train('word-segmentation_' + model_name + '.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7553101104502974\n"
     ]
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('word-segmentation_' + model_name + '.crfsuite')\n",
    "score = 0\n",
    "for word in word_for_test:\n",
    "    w = word.replace(\" \", \"\")\n",
    "    prediction = tagger.tag(create_word_features(w))\n",
    "    complete = \"\"\n",
    "    for i, p in enumerate(prediction):\n",
    "        if int(p) >= 1:\n",
    "            complete += \" \" + w[i]\n",
    "        else:\n",
    "            complete += w[i]\n",
    "    if complete == ' '.join(word_dict_for_test[word].affix_list):\n",
    "        score += 1\n",
    "    else:\n",
    "        continue\n",
    "#         print(f'{word} -> {complete}, {word_dict_for_test[word].affix_list}')\n",
    "print(score / len(word_for_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('word-segmentation_' + model_name + '.crfsuite')\n",
    "\n",
    "def segment_word_(word):\n",
    "    w = word.replace(\" \", \"\")\n",
    "    prediction = tagger.tag(create_word_features(w))\n",
    "    complete = \"\"\n",
    "    for i, p in enumerate(prediction):\n",
    "        if int(p) >= 1:\n",
    "            complete += \" \" + w[i]\n",
    "        else:\n",
    "            complete += w[i]\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'segmentation'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_word_('segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = [0.86, 0.86, 0.83]\n",
    "ev = [0.70, 0.75, '_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['5-fold cross validation'] = ff\n",
    "d['acc on the other dataset'] = ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5-fold cross validation</th>\n",
       "      <th>acc on the other dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.83</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   5-fold cross validation acc on the other dataset\n",
       "0                     0.86                      0.7\n",
       "1                     0.86                     0.75\n",
       "2                     0.83                        _"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
